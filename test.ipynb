{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created virtual environment CPython3.10.1.final.0-64 in 10109ms\n",
      "  creator CPython3Windows(dest=C:\\Users\\muhammad.itqan\\Desktop\\Audio Assignment\\venv, clear=False, no_vcs_ignore=False, global=False)\n",
      "  seeder FromAppData(download=False, pip=bundle, setuptools=bundle, wheel=bundle, via=copy, app_data_dir=C:\\Users\\muhammad.itqan\\AppData\\Local\\pypa\\virtualenv)\n",
      "    added seed packages: pip==24.2, setuptools==75.1.0, wheel==0.44.0\n",
      "  activators BashActivator,BatchActivator,FishActivator,NushellActivator,PowerShellActivator,PythonActivator\n"
     ]
    }
   ],
   "source": [
    "#!python -m virtualenv venv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install audiomentations -q\n",
    "!pip install librosa -q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "from audiomentations import Compose, AddGaussianNoise, AddBackgroundNoise\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd \"C:\\Users\\muhammad.itqan\\Desktop\\Audio Assignment\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\muhammad.itqan\\Desktop\\Audio Assignment\\venv\\lib\\site-packages\\audiomentations\\core\\transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Load the audio file\n",
    "audio, sample_rate = sf.read(\"audio_1.mp3\")\n",
    "\n",
    "# Define the augmentation pipeline\n",
    "augment = Compose([\n",
    "    AddGaussianNoise(min_amplitude=0.001, max_amplitude=0.015, p=1.0)\n",
    "])\n",
    "\n",
    "# Apply augmentation\n",
    "augmented_audio = augment(samples=audio, sample_rate=sample_rate)\n",
    "\n",
    "# Save the augmented audio\n",
    "sf.write(\"example_with_noise.mp3\", augmented_audio, sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "def load_audio(filename):\n",
    "    data, samplerate = sf.read(filename)\n",
    "    return data, samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Preprocess the audio (convert to spectrogram)\n",
    "def preprocess_audio(data, sr):\n",
    "    # Convert to Mel spectrogram using librosa\n",
    "    mel_spec = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128)\n",
    "    return mel_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, samplerate = load_audio('example_with_noise.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48000"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samplerate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def downsample(data, samplerate, target_sr = 16000):\n",
    "#     audio_data_downsampled = librosa.resample(data, orig_sr=samplerate, target_sr=target_sr)\n",
    "\n",
    "#     # Adjust the new sample rate for your model\n",
    "#     new_samplerate = target_sr\n",
    "#     return audio_data_downsampled, new_samplerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data, input_sample_rate = load_audio('example_with_noise.mp3',)\n",
    "#spec = preprocess_audio(input_data, input_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trim audio to the first 30 seconds\n",
    "max_length = 2 * input_sample_rate  \n",
    "audio_data_trimmed = input_data[:max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Save the audio as a WAV file\n",
    "sf.write('denoised_audio_cut.wav', audio_data_trimmed, input_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec = preprocess_audio(audio_data_trimmed, input_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 188)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_spec = librosa.feature.inverse.mel_to_stft(spec, sr=input_sample_rate)\n",
    "\n",
    "# Step 2: Phase recovery using Griffin-Lim\n",
    "\n",
    "# Griffin-Lim algorithm for phase recovery (to approximate the audio waveform)\n",
    "reconstructed_audio = librosa.istft(linear_spec)\n",
    "\n",
    "sf.write('denoised_audio_inversed.wav', reconstructed_audio, input_sample_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(96000,)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_data_trimmed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torch\n",
      "  Downloading torch-2.5.1-cp310-cp310-win_amd64.whl.metadata (28 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Downloading filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torch) (4.12.2)\n",
      "Collecting networkx (from torch)\n",
      "  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting jinja2 (from torch)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting fsspec (from torch)\n",
      "  Downloading fsspec-2024.10.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting sympy==1.13.1 (from torch)\n",
      "  Downloading sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy==1.13.1->torch)\n",
      "  Downloading mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->torch)\n",
      "  Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl.metadata (4.1 kB)\n",
      "Downloading torch-2.5.1-cp310-cp310-win_amd64.whl (203.1 MB)\n",
      "   ---------------------------------------- 0.0/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/203.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.3/203.1 MB 5.1 MB/s eta 0:00:40\n",
      "   - -------------------------------------- 6.3/203.1 MB 14.8 MB/s eta 0:00:14\n",
      "   -- ------------------------------------- 12.8/203.1 MB 20.1 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 20.2/203.1 MB 23.6 MB/s eta 0:00:08\n",
      "   ----- ---------------------------------- 28.0/203.1 MB 26.1 MB/s eta 0:00:07\n",
      "   ------ --------------------------------- 34.6/203.1 MB 27.1 MB/s eta 0:00:07\n",
      "   -------- ------------------------------- 41.4/203.1 MB 28.0 MB/s eta 0:00:06\n",
      "   --------- ------------------------------ 48.5/203.1 MB 28.6 MB/s eta 0:00:06\n",
      "   ----------- ---------------------------- 56.1/203.1 MB 29.3 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 62.9/203.1 MB 29.5 MB/s eta 0:00:05\n",
      "   ------------- -------------------------- 70.5/203.1 MB 30.0 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 77.6/203.1 MB 30.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 84.7/203.1 MB 30.7 MB/s eta 0:00:04\n",
      "   ------------------ --------------------- 91.8/203.1 MB 30.8 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 98.8/203.1 MB 30.9 MB/s eta 0:00:04\n",
      "   -------------------- ------------------ 105.6/203.1 MB 30.9 MB/s eta 0:00:04\n",
      "   --------------------- ----------------- 113.2/203.1 MB 31.2 MB/s eta 0:00:03\n",
      "   ----------------------- --------------- 120.1/203.1 MB 31.3 MB/s eta 0:00:03\n",
      "   ------------------------ -------------- 126.9/203.1 MB 31.3 MB/s eta 0:00:03\n",
      "   ------------------------- ------------- 130.5/203.1 MB 30.9 MB/s eta 0:00:03\n",
      "   -------------------------- ------------ 136.6/203.1 MB 30.7 MB/s eta 0:00:03\n",
      "   --------------------------- ----------- 143.9/203.1 MB 30.6 MB/s eta 0:00:02\n",
      "   ---------------------------- ---------- 151.0/203.1 MB 30.8 MB/s eta 0:00:02\n",
      "   ------------------------------ -------- 158.6/203.1 MB 31.0 MB/s eta 0:00:02\n",
      "   ------------------------------- ------- 164.9/203.1 MB 31.1 MB/s eta 0:00:02\n",
      "   -------------------------------- ------ 171.4/203.1 MB 31.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ---- 179.0/203.1 MB 31.2 MB/s eta 0:00:01\n",
      "   ----------------------------------- --- 186.1/203.1 MB 31.3 MB/s eta 0:00:01\n",
      "   ------------------------------------- - 193.2/203.1 MB 31.3 MB/s eta 0:00:01\n",
      "   --------------------------------------  200.0/203.1 MB 31.4 MB/s eta 0:00:01\n",
      "   --------------------------------------  202.9/203.1 MB 31.5 MB/s eta 0:00:01\n",
      "   --------------------------------------- 203.1/203.1 MB 30.5 MB/s eta 0:00:00\n",
      "Downloading sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "   ---------------------------------------- 0.0/6.2 MB ? eta -:--:--\n",
      "   ---------------------------------------- 6.2/6.2 MB 38.0 MB/s eta 0:00:00\n",
      "Downloading filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading fsspec-2024.10.0-py3-none-any.whl (179 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Downloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.7/1.7 MB 45.7 MB/s eta 0:00:00\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-win_amd64.whl (15 kB)\n",
      "Downloading mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "   ---------------------------------------- 0.0/536.2 kB ? eta -:--:--\n",
      "   --------------------------------------- 536.2/536.2 kB 18.3 MB/s eta 0:00:00\n",
      "Installing collected packages: mpmath, sympy, networkx, MarkupSafe, fsspec, filelock, jinja2, torch\n",
      "Successfully installed MarkupSafe-3.0.2 filelock-3.16.1 fsspec-2024.10.0 jinja2-3.1.4 mpmath-1.3.0 networkx-3.4.2 sympy-1.13.1 torch-2.5.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvision\n",
      "  Downloading torchvision-0.20.1-cp310-cp310-win_amd64.whl.metadata (6.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: torch==2.5.1 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torchvision) (2.5.1)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torchvision) (11.0.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (4.12.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from torch==2.5.1->torchvision) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from sympy==1.13.1->torch==2.5.1->torchvision) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from jinja2->torch==2.5.1->torchvision) (3.0.2)\n",
      "Downloading torchvision-0.20.1-cp310-cp310-win_amd64.whl (1.6 MB)\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/1.6 MB ? eta -:--:--\n",
      "   ------------- -------------------------- 0.5/1.6 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.6/1.6 MB 5.6 MB/s eta 0:00:00\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.20.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tqdm\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\muhammad.itqan\\desktop\\audio assignment\\venv\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm\n",
      "Successfully installed tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import zipfile\n",
    "from math import atan2, cos, sin, sqrt, pi, log\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from numpy import linalg as LA\n",
    "from torch import optim, nn\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv_op = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv_op(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DownSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down = self.conv(x)\n",
    "        p = self.pool(down)\n",
    "\n",
    "        return down, p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UpSample(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.ConvTranspose1d(in_channels, in_channels//2, kernel_size=2, stride=2)\n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        x = torch.cat([x1, x2], 1)\n",
    "        return self.conv(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.down_convolution_1 = DownSample(in_channels, 64)\n",
    "        self.down_convolution_2 = DownSample(64, 128)\n",
    "        self.down_convolution_3 = DownSample(128, 256)\n",
    "        self.down_convolution_4 = DownSample(256, 512)\n",
    "\n",
    "        self.bottle_neck = DoubleConv(512, 1024)\n",
    "\n",
    "        self.up_convolution_1 = UpSample(1024, 512)\n",
    "        self.up_convolution_2 = UpSample(512, 256)\n",
    "        self.up_convolution_3 = UpSample(256, 128)\n",
    "        self.up_convolution_4 = UpSample(128, 64)\n",
    "\n",
    "        self.out = nn.Conv1d(in_channels=64, out_channels=1, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        down_1, p1 = self.down_convolution_1(x)\n",
    "        down_2, p2 = self.down_convolution_2(p1)\n",
    "        down_3, p3 = self.down_convolution_3(p2)\n",
    "        down_4, p4 = self.down_convolution_4(p3)\n",
    "\n",
    "        b = self.bottle_neck(p4)\n",
    "\n",
    "        up_1 = self.up_convolution_1(b, down_4)\n",
    "        up_2 = self.up_convolution_2(up_1, down_3)\n",
    "        up_3 = self.up_convolution_3(up_2, down_2)\n",
    "        up_4 = self.up_convolution_4(up_3, down_1)\n",
    "\n",
    "        out = self.out(up_4)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_data_trimmed = np.expand_dims(audio_data_trimmed,axis = 0)\n",
    "tensor_1 = torch.tensor(np.expand_dims(audio_data_trimmed,axis =0),dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 512])\n"
     ]
    }
   ],
   "source": [
    "input_image = torch.rand((1,1,512))\n",
    "\n",
    "output = model(input_image)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96000])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(tensor_1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
