{"cells":[{"cell_type":"markdown","metadata":{"id":"AC6l38I1M93D"},"source":["## GenAI Assignment 1\n","### Github https://github.com/itqan-abdullah/Audio-Assignment\n","## OverFitting a Model to Remove Piano Background Sound"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2995,"status":"ok","timestamp":1733055218047,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"4WAqjOXwgNt6","outputId":"dd301e67-e337-47be-cd23-00ac2aa6de5a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"ZTnGHG5nNdDu"},"source":["#### Installing and Loading Libraries"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":20562,"status":"ok","timestamp":1733055238606,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"IvdG-swufPXY"},"outputs":[],"source":["!pip install audiomentations -q\n","!pip install librosa -q\n","!pip install torch -q\n","!pip install torchvision -q\n","!pip install opencv-python -q\n","!pip install tqdm -q"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":518,"status":"ok","timestamp":1733055239117,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"4-V-BRNbfPXZ"},"outputs":[],"source":["import soundfile as sf\n","from audiomentations import Compose, AddGaussianNoise, AddBackgroundNoise\n","import numpy as np\n","import librosa\n","\n","\n","import copy\n","import os\n","import random\n","import shutil\n","import zipfile\n","from math import atan2, cos, sin, sqrt, pi, log\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from numpy import linalg as LA\n","from torch import optim, nn\n","from torch.utils.data import DataLoader, random_split\n","from torch.utils.data.dataset import Dataset\n","from torchvision import transforms\n","from tqdm import tqdm\n","import torch.nn.functional as F\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1733055239118,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"5EqZ_qYLfPXa","outputId":"421c3873-4763-4b19-90c8-46ff90490aee"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/Audio-Assignment-main\n"]}],"source":["%cd \"/content/drive/MyDrive/Audio-Assignment-main\""]},{"cell_type":"markdown","metadata":{"id":"cD1AY-2yO1QD"},"source":["#### An audio loader function that ensures 16000 sampling rate"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1733055239569,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"X_E5PYx0fPXb"},"outputs":[],"source":["def load_audio(filename, target_samplerate = 16000):\n","    # Load the audio file\n","    data, original_samplerate = sf.read(filename)\n","\n","    # If the original sampling rate is different from the target, resample\n","    if original_samplerate != target_samplerate:\n","        data = librosa.resample(data, orig_sr=original_samplerate, target_sr=target_samplerate)\n","\n","    return data, target_samplerate"]},{"cell_type":"markdown","metadata":{"id":"5-xF8AvDOY_P"},"source":["#### A helper function that overlays piano sound to the audio Itqan has recorded\n"]},{"cell_type":"code","execution_count":41,"metadata":{"executionInfo":{"elapsed":1402,"status":"ok","timestamp":1733055987202,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"tmLtMJU9fPXa"},"outputs":[],"source":["# Load the audio file\n","audio, sample_rate = sf.read(\"audio_1.mp3\")\n","\n","# Augmentation with background noise\n","augment = Compose([\n","    AddBackgroundNoise(sounds_path=\"piano.mp3\", min_snr_db=10, max_snr_db=30, p=1.0)\n","])\n","\n","# Apply the augmentation\n","augmented_audio = augment(samples=audio, sample_rate=sample_rate)\n","\n","# Save the augmented audio\n","sf.write(\"audio_with_piano.mp3\", augmented_audio, sample_rate)"]},{"cell_type":"markdown","metadata":{"id":"wWayoVazPmEH"},"source":["# Model Preparation\n","Now that we an input and a label, we need a model for background sound removal. Taking inspiration from image to image models, we decided to adopt UNET that maps aor mixed input audio to one with background piano music removed. A UNET progressively shortens the spacialilty of the data, increases channels to a certain extent and then expands it with the help of transpose convolutions and skip connections to map it to the dimensions of the output.\n","For this we need:\n","1. Double Convolutions\n","2. DownSampling Layers\n","3. UpSampling Layers\n"]},{"cell_type":"code","execution_count":43,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733056031616,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"6QufbqiofPXg"},"outputs":[],"source":["class DoubleConv(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv_op = nn.Sequential(\n","            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True),\n","            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n","            nn.ReLU(inplace=True)\n","        )\n","\n","    def forward(self, x):\n","        return self.conv_op(x)"]},{"cell_type":"code","execution_count":44,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733056033263,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"7sxLnrk1fPXg"},"outputs":[],"source":["class DownSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.conv = DoubleConv(in_channels, out_channels)\n","        self.pool = nn.MaxPool1d(kernel_size=2, stride=2)\n","\n","    def forward(self, x):\n","        down = self.conv(x)\n","        p = self.pool(down)\n","\n","        return down, p"]},{"cell_type":"code","execution_count":45,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733056033786,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"p5w_UQP5fPXg"},"outputs":[],"source":["class UpSample(nn.Module):\n","    def __init__(self, in_channels, out_channels):\n","        super().__init__()\n","        self.up = nn.ConvTranspose1d(in_channels, in_channels//2, kernel_size=2, stride=2)\n","        self.conv = DoubleConv(in_channels, out_channels)\n","\n","    def forward(self, x1, x2):\n","        x1 = self.up(x1)\n","        x = torch.cat([x1, x2], 1)\n","        return self.conv(x)"]},{"cell_type":"markdown","metadata":{"id":"urDcGGZvQUnm"},"source":["## Consolidating the model"]},{"cell_type":"code","execution_count":46,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733056033786,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"35rfxF37fPXg"},"outputs":[],"source":["class UNet(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        self.down_convolution_1 = DownSample(in_channels, 64)\n","        self.down_convolution_2 = DownSample(64, 128)\n","        self.down_convolution_3 = DownSample(128, 256)\n","        self.down_convolution_4 = DownSample(256, 512)\n","\n","        self.bottle_neck = DoubleConv(512, 1024)\n","\n","        self.up_convolution_1 = UpSample(1024, 512)\n","        self.up_convolution_2 = UpSample(512, 256)\n","        self.up_convolution_3 = UpSample(256, 128)\n","        self.up_convolution_4 = UpSample(128, 64)\n","\n","        self.out = nn.Conv1d(in_channels=64, out_channels=1, kernel_size=1)\n","\n","    def forward(self, x):\n","        down_1, p1 = self.down_convolution_1(x)\n","        down_2, p2 = self.down_convolution_2(p1)\n","        down_3, p3 = self.down_convolution_3(p2)\n","        down_4, p4 = self.down_convolution_4(p3)\n","\n","        b = self.bottle_neck(p4)\n","\n","        up_1 = self.up_convolution_1(b, down_4)\n","        up_2 = self.up_convolution_2(up_1, down_3)\n","        up_3 = self.up_convolution_3(up_2, down_2)\n","        up_4 = self.up_convolution_4(up_3, down_1)\n","\n","        out = self.out(up_4)\n","        return out"]},{"cell_type":"markdown","metadata":{"id":"UWGvNFoTQiSq"},"source":["### We also need an audio dataset class and a training function"]},{"cell_type":"code","execution_count":47,"metadata":{"executionInfo":{"elapsed":409,"status":"ok","timestamp":1733056038014,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"kxNDdNaafPXi"},"outputs":[],"source":["\n","class AudioDataset(torch.utils.data.Dataset):\n","    def __init__(self, noisy_audio_path = 'audio_with_piano.mp3', clean_audio_path = 'audio_1.mp3', sample_rate=16000, duration=2):\n","        self.sample_rate = sample_rate\n","        self.samples_per_segment = int(sample_rate * duration)  # 2 seconds = 32000 samples\n","\n","        # Load audio files\n","        noisy_audio = torch.tensor(\n","            load_audio(noisy_audio_path)[0][:100000], dtype=torch.float\n","        )\n","        clean_audio = torch.tensor(\n","            load_audio(clean_audio_path)[0][:100000], dtype=torch.float\n","        )\n","\n","        # Ensure both audio tensors are the same length\n","        min_length = min(len(noisy_audio), len(clean_audio))\n","        noisy_audio = noisy_audio[:min_length]\n","        clean_audio = clean_audio[:min_length]\n","\n","        # Split into segments\n","        self.noisy_segments = noisy_audio.unfold(0, self.samples_per_segment, self.samples_per_segment)\n","        self.clean_segments = clean_audio.unfold(0, self.samples_per_segment, self.samples_per_segment)\n","\n","    def __len__(self):\n","        return self.noisy_segments.size(0)\n","    def __getitem__(self, idx):\n","        # Add necessary dimensions for (batch_size, channels, time_steps)\n","        noisy_segment = self.noisy_segments[idx].unsqueeze(0)  # Shape: (1, 1, 32000)\n","        clean_segment = self.clean_segments[idx].unsqueeze(0)  # Shape: (1, 1, 32000)\n","        return noisy_segment, clean_segment\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vyAayRNYQpRw"},"outputs":[],"source":["# Training loop function\n","def train_model(model, dataloader, criterion, optimizer, num_epochs, device):\n","    model.to(device)\n","    for epoch in range(num_epochs):\n","        model.train()\n","        running_loss = 0.0\n","\n","        for noisy, clean in dataloader:\n","            noisy = noisy.to(device)  # Move input to device\n","            clean = clean.to(device)  # Move target to device\n","\n","            optimizer.zero_grad()  # Zero out previous gradients\n","\n","            # Forward pass\n","            denoised = model(noisy)\n","\n","            # Compute loss\n","            loss = criterion(denoised, clean)\n","\n","            # Backward pass and optimization\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item() * noisy.size(0)  # Accumulate batch loss\n","\n","        epoch_loss = running_loss / len(dataloader.dataset)\n","        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"]},{"cell_type":"code","execution_count":48,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733056039601,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"xdG9cztPqD2f"},"outputs":[],"source":["dataset = AudioDataset()\n","dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1733056041545,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"QSC0I6JdqOEZ"},"outputs":[],"source":["model = UNet(1)\n","criterion = nn.MSELoss()  # Loss function\n","optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)  # Optimizer"]},{"cell_type":"markdown","metadata":{"id":"zdEQVUoYQ7Sc"},"source":["## Training"]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25340,"status":"ok","timestamp":1733056068348,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"RgT6qYAHDJUY","outputId":"94308759-6b17-4b77-dff4-e842e7a4113c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/100, Loss: 0.0037\n","Epoch 2/100, Loss: 0.0034\n","Epoch 3/100, Loss: 0.0033\n","Epoch 4/100, Loss: 0.0032\n","Epoch 5/100, Loss: 0.0030\n","Epoch 6/100, Loss: 0.0029\n","Epoch 7/100, Loss: 0.0026\n","Epoch 8/100, Loss: 0.0022\n","Epoch 9/100, Loss: 0.0019\n","Epoch 10/100, Loss: 0.0014\n","Epoch 11/100, Loss: 0.0010\n","Epoch 12/100, Loss: 0.0007\n","Epoch 13/100, Loss: 0.0006\n","Epoch 14/100, Loss: 0.0006\n","Epoch 15/100, Loss: 0.0006\n","Epoch 16/100, Loss: 0.0005\n","Epoch 17/100, Loss: 0.0003\n","Epoch 18/100, Loss: 0.0003\n","Epoch 19/100, Loss: 0.0002\n","Epoch 20/100, Loss: 0.0002\n","Epoch 21/100, Loss: 0.0002\n","Epoch 22/100, Loss: 0.0002\n","Epoch 23/100, Loss: 0.0002\n","Epoch 24/100, Loss: 0.0002\n","Epoch 25/100, Loss: 0.0002\n","Epoch 26/100, Loss: 0.0002\n","Epoch 27/100, Loss: 0.0002\n","Epoch 28/100, Loss: 0.0002\n","Epoch 29/100, Loss: 0.0002\n","Epoch 30/100, Loss: 0.0002\n","Epoch 31/100, Loss: 0.0002\n","Epoch 32/100, Loss: 0.0002\n","Epoch 33/100, Loss: 0.0002\n","Epoch 34/100, Loss: 0.0002\n","Epoch 35/100, Loss: 0.0002\n","Epoch 36/100, Loss: 0.0001\n","Epoch 37/100, Loss: 0.0001\n","Epoch 38/100, Loss: 0.0001\n","Epoch 39/100, Loss: 0.0001\n","Epoch 40/100, Loss: 0.0001\n","Epoch 41/100, Loss: 0.0001\n","Epoch 42/100, Loss: 0.0001\n","Epoch 43/100, Loss: 0.0001\n","Epoch 44/100, Loss: 0.0001\n","Epoch 45/100, Loss: 0.0001\n","Epoch 46/100, Loss: 0.0001\n","Epoch 47/100, Loss: 0.0001\n","Epoch 48/100, Loss: 0.0001\n","Epoch 49/100, Loss: 0.0001\n","Epoch 50/100, Loss: 0.0001\n","Epoch 51/100, Loss: 0.0001\n","Epoch 52/100, Loss: 0.0001\n","Epoch 53/100, Loss: 0.0001\n","Epoch 54/100, Loss: 0.0001\n","Epoch 55/100, Loss: 0.0001\n","Epoch 56/100, Loss: 0.0001\n","Epoch 57/100, Loss: 0.0001\n","Epoch 58/100, Loss: 0.0001\n","Epoch 59/100, Loss: 0.0001\n","Epoch 60/100, Loss: 0.0001\n","Epoch 61/100, Loss: 0.0001\n","Epoch 62/100, Loss: 0.0001\n","Epoch 63/100, Loss: 0.0001\n","Epoch 64/100, Loss: 0.0001\n","Epoch 65/100, Loss: 0.0001\n","Epoch 66/100, Loss: 0.0001\n","Epoch 67/100, Loss: 0.0001\n","Epoch 68/100, Loss: 0.0001\n","Epoch 69/100, Loss: 0.0001\n","Epoch 70/100, Loss: 0.0001\n","Epoch 71/100, Loss: 0.0001\n","Epoch 72/100, Loss: 0.0001\n","Epoch 73/100, Loss: 0.0001\n","Epoch 74/100, Loss: 0.0001\n","Epoch 75/100, Loss: 0.0001\n","Epoch 76/100, Loss: 0.0001\n","Epoch 77/100, Loss: 0.0001\n","Epoch 78/100, Loss: 0.0001\n","Epoch 79/100, Loss: 0.0001\n","Epoch 80/100, Loss: 0.0001\n","Epoch 81/100, Loss: 0.0001\n","Epoch 82/100, Loss: 0.0001\n","Epoch 83/100, Loss: 0.0001\n","Epoch 84/100, Loss: 0.0001\n","Epoch 85/100, Loss: 0.0001\n","Epoch 86/100, Loss: 0.0001\n","Epoch 87/100, Loss: 0.0001\n","Epoch 88/100, Loss: 0.0001\n","Epoch 89/100, Loss: 0.0001\n","Epoch 90/100, Loss: 0.0001\n","Epoch 91/100, Loss: 0.0001\n","Epoch 92/100, Loss: 0.0001\n","Epoch 93/100, Loss: 0.0001\n","Epoch 94/100, Loss: 0.0001\n","Epoch 95/100, Loss: 0.0001\n","Epoch 96/100, Loss: 0.0001\n","Epoch 97/100, Loss: 0.0001\n","Epoch 98/100, Loss: 0.0001\n","Epoch 99/100, Loss: 0.0001\n","Epoch 100/100, Loss: 0.0001\n"]}],"source":["train_model(model, dataloader, criterion, optimizer, num_epochs=100, device=device)"]},{"cell_type":"markdown","metadata":{"id":"ajF_3iVoQ-ON"},"source":["# Inference"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":403,"status":"ok","timestamp":1733056191181,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"Ipwj7mOtqk9B"},"outputs":[],"source":["model.eval()\n","\n","# Example input (batch of size 1, 10 features)\n","noisy_audio = 'audio_with_piano.mp3'\n","input_data = torch.tensor(load_audio(noisy_audio)[0][:100000][np.newaxis, np.newaxis, :],dtype= torch.float).to(device)\n","\n","with torch.no_grad():\n","  out2 = model(input_data)\n","out2 = np.array(out2.cpu().squeeze())\n","sf.write('model_out_1.mp3', out2, 16000)"]},{"cell_type":"markdown","metadata":{"id":"PjHjXOHnMgl1"},"source":["# Inputting the output of the model into it again"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":511,"status":"ok","timestamp":1733056179207,"user":{"displayName":"Taqqi Raja","userId":"14440674840393289689"},"user_tz":-300},"id":"6MwR-A_wMg4E"},"outputs":[],"source":["with torch.no_grad():\n","  out2 = model(input_data)\n","  out2 = model(out2)\n","out2 = np.array(out2.cpu().squeeze())\n","sf.write('model_out_2.mp3', out2, 16000)"]},{"cell_type":"markdown","metadata":{"id":"nTLcfHtyRWLN"},"source":["# Conclusion\n","The model can remove piano successfully from the overfitted data. Additionally, it also doesn't remove necessary data if given a sample with no background audio as in the case of inputting the output of the model again into it."]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":0}
